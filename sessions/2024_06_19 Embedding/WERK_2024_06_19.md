---
theme: werkstatt
---
## Künstliche Intelligenz & Manipulation

#### Bauhaus Digitalwerkstatt

---
<!-- slide bg="pink" -->

### News Alert

> ![News Image](https://www.sueddeutsche.de/2022/06/14/cfafe66b-c553-48eb-b536-1b80d0276248.jpeg?q=60&fm=avif&width=1536&rect=0%2C0%2C1601%2C900)
> 
> ##### Los Angeles: Verkehr durch KI verbessern
> [KI und Verkehr](https://www.tagesschau.de/wirtschaft/technologie/ki-verkehr-la-100.html)

---

<!-- slide bg="yellow" -->

### Quiz

>- Durch welche Zelle unseres Körpers ist der Perzeptron Algorithmus inspiriert?
>- Beim Lernen ändert der Perzeptron Algorithmus die Gewichte oder die Eingaben?
>- Der Perzeptron Algorithmus verarbeitet mehrere Eingabeknoten. Warum hat der erste Knoten immer den Wert 1?
>- Was kann schiefgehen, wenn wir Objekte aus der realen Welt in einen Feature Space überführen?

---
<!-- slide bg="yellow" -->

![|600](https://www.statworx.com/wp-content/uploads/2017/12/Blog_Rosenblatt-Perzeptron_perceptron.jpg)

---
<!-- slide bg="yellow" -->

![[Pasted image 20240529172608.png]]

---


---
<!-- slide bg="yellow" -->
## Perzeptrons am Limit

---

![|600](https://www.statworx.com/wp-content/uploads/2017/12/Blog_Rosenblatt-Perzeptron_perceptron.jpg)

---
<!-- slide data-background-iframe="https://ludattel.de/Perceptron_AND/"  data-background-interactive -->

---

```js
function draw() {
  background(255);
  
  // Hole dir das aktuelle Beispiel über rounds
  inputs = examples[rounds][0]
  target = examples[rounds][1]
  
  // Berechne die gewichtete Summe
  summe = 0
  for (var i = 0; i < weights.length; i++) {
    summe += inputs[i]*weights[i]
  }

  // Lege dich auf das Ergebnis fest
  result = 0
  if (summe >= 0)
    result = 1

  // Zeichne das fertige Perzeptron
  draw_perceptron(inputs, weights, 0.5, result, target)
  

  // Ändere die Gewichte wenn die Berechnung nicht mit der Erwartung übereinstimmt
  for (var i = 0; i < weights.length; i++) {
    weights[i] += learning_rate * inputs[i] * (target - result)
  }
  
```
---

```js
  // Hole dir das aktuelle Beispiel über rounds
  inputs = examples[rounds][0]
  target = examples[rounds][1]
```
- die Runden Variable ändert sich mit der Anzahl der Beispiele (`0, 1, 2, 3, 0, 1, 2 ...`) und ist für uns wie ein Taktgeber / Metronom

```js
  rounds = (rounds + 1) % examples.length
```

---

```js
// Berechne die gewichtete Summe
  summe = 0
  for (var i = 0; i < weights.length; i++) {
    summe += inputs[i]*weights[i]
  }
```
- so sagen wir das Ergebnis mit hilfe unseres Modells vorher (**Prediction**)
- das ist der vorwärts-Aufruf (**Forward-Call**)

---

```js
  // Zeichne das fertige Perzeptron
  draw_perceptron(inputs, weights, 0.5, result, target)
  
```

- das Zeichnen haben wir hier ausgelagert und ist für das Verstehen des Algorithmus nicht wichtig

---

```js

  // Ändere die Gewichte wenn die Berechnung nicht mit der Erwartung übereinstimmt
  for (var i = 0; i < weights.length; i++) {
    weights[i] += learning_rate * inputs[i] * (target - result)
  }
```
- jetzt ändern wir die gewichte wenn wir mit unserer Vorhersage falsch lagen 
- Rückwärts Aufruf (**Backward Call**)
- hier findet das eigentliche Lernen statt

---

- [Link zum Perzeptron Code](https://editor.p5js.org/henicosa/sketches/psxCJWXlo)

---
### Lineare Separierbarkeit 
#### (bei der Klassifizierung)

<split even>
![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Separability_NO.svg/238px-Separability_NO.svg.png)

![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Separability_YES.svg/238px-Separability_YES.svg.png)
</split>
---

* bei 2-dimensionalen Inputs, können die Inputs als Punkte in einem 2-dimensionales Koordinatensystem dargestellt werden, hierbei ist die **lineare** Trennbarkeit eine **Linie**
* bei 3-dimensionalen Inputs Koordinatensystem dann eine Ebene

---

### Das logische "Oder" (or)

| $a$ | $b$ | $a \lor b$ |
| :-- | :-- | :--------- |
| 0   | 0   | 0          |
| 0   | 1   | 1          |
| 1   | 0   | 1          |
| 1   | 1   | 1          |

---

### Das logische "Exklusive Oder" (xor)

| $a$ | $b$ | $a \oplus b$ |
| :-- | :-- | :----------- |
| 0   | 0   | 0            |
| 0   | 1   | 1            |
| 1   | 0   | 1            |
| 1   | 1   | 0            |



---
#### Das XOR-Problem

![Image](https://methpsy.elearning.psych.tu-dresden.de/mediawiki/images/3/37/Lineare_separierbarkeit.PNG)

---
<!-- slide bg="blue" -->

# Pause

---

<!-- slide bg="yellow" -->

## Word Embeddings
#### Wörter als Richtungen denken

---

![[Pasted image 20240529172608.png]]

---
### Was ist Word Embedding? 

- **Definition**: Word Embedding ist eine Technik, um Wörter in Vektoren zu transformieren, die die semantische Bedeutung der Wörter in einem kontinuierlichen Vektorraum repräsentieren. 
- **Ziel**: Wörter, die in ähnlichen Kontexten auftreten, sollten ähnliche Vektoren haben. 
- **Beispiel**: Die Wörter "König" und "Königin" sollten ähnliche Vektoren haben. 

---
### Wie wird Word Embedding gemacht? 
- **Datenquelle**: Große Textkorpora, z.B. Wikipedia, Nachrichtenartikel. 
- **Techniken**: 
	- **Maschinelles Lernen**: 
		- **Word2Vec**: Trainiert ein neuronales Netzwerk, um Vektoren zu generieren.  
	- **Nicht-maschinelles Lernen**: 
		- **Co-Occurrence Matrix**: Einfache Zählmethoden der Wortnachbarschaften. 

--

- **Trainingsprozess**: 
	- Eingabe: Wortkontexte aus Textkorpus. 
	- Ausgabe: Vektorrepräsentationen der Wörter.

---
### Praktische Beispiele 
- **Synonym-Suche**: 
	- Beispiel: "Auto" und "Fahrzeug" sollten ähnliche Vektoren haben. 
- **Analogie-Bildung**: 
	- Beispiel: "König" - "Mann" + "Frau" ≈ "Königin". 
- **Textklassifikation**: 
	- Beispiel: Sentiment-Analyse von Produktbewertungen.
---
### Wörter auf einer Landkarte

![](https://www.publicdomainpictures.net/pictures/210000/velka/treasure-map.jpg)

---

- [InfiniteCraft](https://neal.fun/infinite-craft/)


---
<!-- slide bg="blue" -->

# Ende
